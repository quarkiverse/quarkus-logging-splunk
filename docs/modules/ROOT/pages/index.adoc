= Quarkus logging splunk

== Introduction

https://www.splunk.com/en_us/software/splunk-enterprise.html[Splunk] is a middleware
solution that receives, stores, indexes and finally allows to exploit the logs of an application.

This Quarkus extension provides the support of the official Splunk client library to index log events through the HTTP Event collection, provided by Splunk enterprise solution.

- The official client is an opensource library available https://github.com/splunk/splunk-library-javalogging[here].
- The documentation of HTTP Event collection can be found https://docs.splunk.com/Documentation/Splunk/8.1.1/Data/UsetheHTTPEventCollector[here].

== Installation

If you want to use this extension, you need to add the `quarkus-logging-splunk` extension first.
In your `pom.xml` file, add:

[source,xml]
----
<dependency>
    <groupId>io.quarkiverse.logging.splunk</groupId>
    <artifactId>quarkus-logging-splunk</artifactId>
    <version>{project-version}</version>
</dependency>
----

== Features

The extension can be used transparently with any log frontend used by Quarkus (Log4j, SLF4J, ... ).

=== Log message formatting

In all cases the log message formatter is aligned by default with the one of Quarkus console handler:

[source,properties]
----
quarkus.log.handler.splunk.format="%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
----

This can be adapted in order to avoid duplication with metadata that are passed in a structured way.

=== Log event metadata

The type of metadata depends on the serialization format.

If `quarkus.log.handler.splunk.raw` is enabled or `quarkus.log.handler.splunk.serialization` is `raw`,  there are no per-event metadata.
Only few global metadata shared between all events of a batch are sent via HTTP headers and query parameters.

In other cases, the extension uses structured logging, via JSON serialization.
There are two supported structured formats:

* The `nested` serialization is the default format of Splunk HEC Java client and defines the name of some pre-defined metadata. Combined with `quarkus.log.handler.splunk.format=%s%e` it also support log messages that are themselves JSON.
* The `flat` serialization is a simpler and more generic format, also used by the https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#splunk-hec[OpenTelemetry Splunk HEC exporter].

Some metadata can be indexed by Splunk, see link:++https://docs.splunk.com/Splexicon:Indexedfield++[indexed fields].
The default `_json` source type indexes metadata passed in the `fields` object.

The extension provides the support of the resolution of MDC scoped properties, as defined in https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.0/html/configuration_guide/logging_with_jboss_eap#log_formatters[JBoss supported formatters].

[%header,cols="h,a,a"]
|===
|Serialization format
|`nested`
|`flat`

|HEC metadata
2+|`time` and `host` are always sent.
`source`, `sourcetype`, `index` are sent if not empty.

|Pre-defined metadata
|Only `event.severity` is sent by default.
Other metadata can be added:

* `event.thread` via `quarkus.log.handler.splunk.include-thread-name`
* `event.exception` via `quarkus.log.handler.splunk.include-exception`
* `event.logger` via `quarkus.log.handler.splunk.include-logger-name`
|Only `fields.severity` is sent by default.
The metadata name can be customized via `quarkus.log.handler.splunk.metadataSeverityFieldName`
Other metadata can be added:

* `fields.thread` via `quarkus.log.handler.splunk.include-thread-name`
* `fields.exception` via `quarkus.log.handler.splunk.include-exception`
* `fields.logger` via `quarkus.log.handler.splunk.include-logger-name`
|MDC properties
|Passed via `event.properties`
|Passed via `fields`

|Static metadata
2+|Passed via `fields`

|===

A structured query to Splunk HEC looks like:

curl -k -v -X POST https://localhost:8080/services/collector/event/1.0 -H "Content-type: application/json; profile=\"urn:splunk:event:1.0\"; charset=utf-8" -H "Authorization: Splunk 29fe2838-cab6-4d17-a392-37b7b8f41f75" -d@events.json

.Nested serialization example
[source,json]
----
{
  "time": "1673001538.042",
  "host": "hostname",
  "source": "mysource",
  "sourcetype": "_json",
  "index": "main",
  "event": {
    "message": "2023-01-06 ERROR The log message",
    "logger": "com.acme.MyClass",
    "severity": "ERROR",
    "exception": "java.lang.NullPointerException",
    "properties": {
      "mdc-key": "mdc-value"
    }
  },
  "fields": {
    "key": "static-value"
  }
}
----

.Flat serialization example
[source,json]
----
{
  "time": "1673001538.042",
  "host": "hostname",
  "source": "mysource",
  "index": "main",
  "event": "2023-01-06 ERROR The log message",
  "fields": {
    "severity": "ERROR",
    "mdc-key": "mdc-value",
    "key": "static-value"
  }
}
----

=== Connectivity failures

Batched events that cannot be sent to the Splunk indexer will be logged to stdout:

* Formatted using console handler settings if the console handler is enabled
* Formatted using splunk handler settings otherwise

In any case, the root cause of the failure is always logged to stderr.

=== Asynchronous handler

By default, the log handler is synchronous and only the HTTP requests to HEC endpoint are done asynchronously:

[plantuml, sync, format=svg]
....
participant "Application" as App
participant Slf4j
participant SplunkLogHandler
participant "Splunk library" as Lib
participant "HTTP client" as OkHttp
participant "Splunk HEC" as HEC

group Application thread
App -> Slf4j: info(message)
Slf4j -> SplunkLogHandler: doPublish(record)
SplunkLogHandler -> Lib: send(record)
note left
  synchronized
end note
Lib -> Lib: Add event to batch
alt batch is full
Lib -> OkHttp: enqueue(HTTP request)
OkHttp --> Lib
end
Lib --> App

end
group HTTP client - multiple connections in parallel mode.
OkHttp -> OkHttp: Peek from queue
OkHttp -> HEC: HTTP POST /services/collector/event/1.0
HEC --> OkHttp: 200
alt status code != 200
OkHttp --> Lib: handle errors
Lib -> Lib: stderr.println
end
end
....

This can be an issue because the Splunk library `#send` is synchronized, so any preprocessing of the batch HTTP request itself happens on the application thread of the log event that triggered the batch to be full (either by reaching `quarkus.log.handler.splunk.batch-size-count` or `quarkus.log.handler.splunk.batch-size-bytes`)

By enabling `quarkus.log.handler.splunk.async=true`, an intermediate event queue is used, which decouples the flushing of the batch from any application thread:

[plantuml, async, format=svg]
....
participant "Application" as App
participant Slf4j
participant AsyncHandler
participant SplunkLogHandler
participant "Splunk library" as Lib
participant "HTTP client" as OkHttp
participant "Splunk HEC" as HEC

group Application thread
App -> Slf4j: info(message)
Slf4j -> AsyncHandler: doPublish(record)
AsyncHandler -> AsyncHandler: Capture MDC
AsyncHandler -> AsyncHandler: Add to queue
AsyncHandler --> App
end

group AsyncHandler single (daemon) thread
AsyncHandler -> AsyncHandler: Peek from queue
AsyncHandler -> SplunkLogHandler: doPublish(record)
SplunkLogHandler -> Lib: send(record)
note left
  synchronized
end note
Lib -> Lib: Add event to batch
alt batch is full
Lib -> OkHttp: enqueue(HTTP request)
OkHttp --> Lib
end
Lib --> SplunkLogHandler
SplunkLogHandler --> AsyncHandler

end
group HTTP client - multiple connections in parallel mode.
OkHttp -> OkHttp: Peek from queue
OkHttp -> HEC: HTTP POST /services/collector/event/1.0
HEC --> OkHttp: 200
alt status code != 200
OkHttp --> Lib: handle errors
Lib -> Lib: stderr.println
end
end
....

By default `quarkus.log.handler.splunk.async.overflow=block`, so applicative threads will block once the queue limit has reached `quarkus.log.handler.splunk.async.queue-length`.

There's no link between `quarkus.log.handler.splunk.async.queue-length` and `quarkus.log.handler.splunk.batch-size-count`.

=== Sequential and parallel modes

The number of events kept in memory for batching purposes is not limited.
After tuning `quarkus.log.handler.splunk.batch-size-count` and `quarkus.log.handler.splunk.batch-size-bytes`, in case the HEC endpoint cannot keep up with the batch throughput, using multiple HTTP connections might help to reduce memory usage on the client.

By setting `quarkus.log.handler.splunk.send-mode=parallel` multiple batches will be sent over the wire in parallel, potentially increasing throughput with the HEC endpoint.

=== Named Splunk log handlers

A named log handler can be configured to manage multiple Splunk configurations for particular log emissions. Like for core Quarkus handlers (*console*, *file* or *syslog*),
Splunk named handlers follow the same configuration:

```
# Global configuration
quarkus.log.handler.splunk.token=12345678-1234-1234-1234-1234567890AB
quarkus.log.handler.splunk.metadata-index=mylogindex

# Splunk named handler configuration, named here MONITORING
quarkus.log.handler.splunk."MONITORING".token=12345678-0000-0000-0000-1234567890AB
quarkus.log.handler.splunk."MONITORING".metadata-index=mystatsindex

# Registration of the custom handler through Quarkus core category management, here monitoring as the logging category
quarkus.log.category."monitoring".handlers=MONITORING
quarkus.log.category."monitoring".use-parent-handlers=false
```

Next to use such logger in actual code, you can rely on annotation or factory:

* With annotation:
```
@LoggerName("monitoring")
Logger monitoringLogger;
```

* With factory:
```
static final Logger monitoringLogger = Logger.getLogger("monitoring");
```

==== Some important considerations
* Every handler is isolated and uses a separate Splunk client and connection pool, which means it has a cost.
* The configuration from the root handler are not inherited by named handlers.
* Use `quarkus.log.category."named-handler".use-parent-handlers=false` is required if you do not want the root handler to also receive log events already sent to named handlers.

== Developer experience

To enhance the developer experience, some integration in the Development mode of Quarkus is provided.

=== Dev service

The extension provides a Dev Service that starts in background a splunk container.
It is deactivated by default, to maintain the compatibility with disabling the splunk extension at runtime.

==== Activation

To activate the dev service, the following needs to be configured with this link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-devservices-enabled[property]:

[source,properties]
----
quarkus.log.handler.splunk.devservices.enabled=true
----

Obviously in "normal" mode (not dev, not test), this has no effect.
The https://quarkus.io/guides/dev-services[Quarkus dev services] framework picks up the configuration and starts the splunk container.
When eventually the container is considered started, some configuration is injected to expose the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-url[random] port on which splunk is listening.
It also injects the following:

[source,properties]
----
quarkus.log.handler.splunk.token=local-dev-token
quarkus.log.handler.splunk.disable-certificate-validation=true
quarkus.log.handler.splunk.enabled=true
----

Namely:

* the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-token[HEC token] configured in the Splunk container at boot
* Splunk enforces HTTPS on endpoints but with self-signed certificates, so we need to link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-disable-certificate-validation[ignore certificate validation]
* forcing the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-enabled[activation] of the `quarkus-logging-splunk` extension when its dev service has been activated at build time.

==== Support of named handlers

Named handlers are supported through additional build time configuration.
Example:

[source,properties]
----
quarkus.log.handler.splunk.devservices.plug-named-handlers.<myhandler>=true
----

link:[Here] is the config property table entry.

The result is that it will override for that named handler the following configuration:

* the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-named-handlers-url[HEC endpoint]
* the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-named-handlers-token[token]
* remove of link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-named-handlers-disable-certificate-validation[certificate verification]
* force link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-named-handlers-enabled[enabled] the splunk log handler

==== Usage in tests

a `quarkus-logging-splunk-test-utils` module proposes some test framework layer allowing access to the link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-devservices-api-url[Splunk API URL].
This is useful to launch searches after the test run to do some assertions on the logs eventually sent to Splunk.
It is definitely not to be used in all tests, as it quite lengthens the test run time.
I.e. the start of splunk takes about 30s.
And there needs to be added some delay after the test run to make sure log entries have been properly propagated to Splunk.

The search in Splunk can be done using its https://dev.splunk.com/enterprise/reference/[API].
We propose a custom QuarkusTestResourceLifecycleManager to inject the URL to the Splunk API (for compatibility with `QuarkusIntegrationTest` when `microprofile-config` injection is disallowed):

.Example of usage of test utils
[source,java]
----
@QuarkusTest
@QuarkusTestResource(LoggingSplunkInjectionTestResource.class) // <1>
class MyQuarkusTest {
    @LoggingSplunkApiUrl // <2>
    String splunkApiUrl;

    @Test
    void test() {
        RestAssured.given()
                .request()
                .formParam("search", "search \"hello splunk\"")
                .formParam("exec_mode", "oneshot")
                .relaxedHTTPSValidation()
                .auth()
                .basic("admin", "admin123") // <3>
                .log()
                .ifValidationFails()
                .post(splunkApiUrl + "/services/search/jobs") // <4>
                .then()
                .statusCode(200)
                .body(containsString("hello splunk"), containsString("mdc-value"));
    }
}
----

<1> The `QuarkusTestResource` to declare, which will also be picked by the potential `IT` test extending this `QuarkusTest` class.
<2> The annotation to use on a `String` field, where the Splunk API will be injected
<3> The default credentials configured in the Splunk container (user admin, password admin123)
<4> The URL injected has the `https://host:port` pattern, so it need to be completed with the actual service path you want to access.

==== Additional configuration

To customize a bit the splunk container started, a few configuration options are given:

* customize the actual container image used through `link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-devservices-image-name[quarkus.log.handler.splunk.devservices.image-name]`
* enforce that in dev mode the splunk instances are shared between runs of microservices run in Dev mode with `link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-devservices-shared[quarkus.log.handler.splunk.devservices.shared]` (boolean).
Default is shared true.
* Add/customize environment variables with `link:#quarkus-log-handler-splunk_quarkus-log-handler-splunk-devservices-container-env-container-env[quarkus.log.handler.splunk.devservices.container-env]`.
Map of key values.

== Extension Configuration Reference

This extension follows the `log handlers` configuration domain that is defined by Quarkus, every configuration property of this extension will belong to the following configuration root : `quarkus.log.handler.splunk`

When present this extension is enabled by default, meaning the client would expect a valid connection to a Splunk indexer and would print an error message for every log created by the application.

So in local environment, the log handler can be disabled with the following property :

[source,properties]
----
quarkus.log.handler.splunk.enabled=false
----

Every configuration property of the extension is overridable at runtime.

include::includes/quarkus-log-handler-splunk.adoc[leveloffset=+1, opts=optional]
