= Quarkus logging splunk

== Introduction

https://www.splunk.com/en_us/software/splunk-enterprise.html[Splunk] is a middleware
solution that receives, stores, indexes and finally allows to exploit the logs of an application.

This Quarkus extension provides the support of the official Splunk client library to index log events through the HTTP Event collection, provided by Splunk enterprise solution.

- The official client is an opensource library available https://github.com/splunk/splunk-library-javalogging[here].
- The documentation of HTTP Event collection can be found https://docs.splunk.com/Documentation/Splunk/8.1.1/Data/UsetheHTTPEventCollector[here].

== Installation

If you want to use this extension, you need to add the `quarkus-logging-splunk` extension first.
In your `pom.xml` file, add:

[source,xml]
----
<dependency>
    <groupId>io.quarkiverse.logging.splunk</groupId>
    <artifactId>quarkus-logging-splunk</artifactId>
</dependency>
----

== Features

The extension can be used transparently with any log frontend used by Quarkus (Log4j, SLF4J, ... ).

=== Log message formatting

In all cases the log message formatter is aligned by default with the one of Quarkus console handler:

[source,properties]
----
quarkus.log.handler.splunk.format="%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
----

This can be adapted in order to avoid duplication with metadata that are passed in a structured way.

=== Log event metadata

The type of metadata depends on the serialization format.

If `quarkus.log.handler.splunk.raw` is enabled or `quarkus.log.handler.splunk.serialization` is `raw`,  there are no per-event metadata.
Only few global metadata shared between all events of a batch are sent via HTTP headers and query parameters.

In other cases, the extension uses structured logging, via JSON serialization.
There are two supported structured formats:

* The `nested` serialization is the default format of Splunk HEC Java client and defines the name of some pre-defined metadata. Combined with `quarkus.log.handler.splunk.format=%s%e` it also support log messages that are themselves JSON.
* The `flat` serialization is a simpler and more generic format, also used by the https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/logs/data-model.md#splunk-hec[OpenTelemetry Splunk HEC exporter].

Some metadata can be indexed by Splunk, see link:++https://docs.splunk.com/Splexicon:Indexedfield++[indexed fields].
The default `_json` source type indexes metadata passed in the `fields` object.

The extension provides the support of the resolution of MDC scoped properties, as defined in https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.0/html/configuration_guide/logging_with_jboss_eap#log_formatters[JBoss supported formatters].

[%header,cols="h,a,a"]
|===
|Serialization format
|`nested`
|`flat`

|HEC metadata
2+|`time` and `host` are always sent.
`source`, `sourcetype`, `index` are sent if not empty.

|Pre-defined metadata
|Only `event.severity` is sent by default.
Other metadata can be added:

* `event.thread` via `quarkus.log.handler.splunk.include-thread-name`
* `event.exception` via `quarkus.log.handler.splunk.include-exception`
* `event.logger` via `quarkus.log.handler.splunk.include-logger-name`
|Only `fields.severity` is sent by default.
The metadata name can be customized via `quarkus.log.handler.splunk.metadataSeverityFieldName`
Other metadata can be added:

* `fields.thread` via `quarkus.log.handler.splunk.include-thread-name`
* `fields.exception` via `quarkus.log.handler.splunk.include-exception`
* `fields.logger` via `quarkus.log.handler.splunk.include-logger-name`
|MDC properties
|Passed via `event.properties`
|Passed via `fields`

|Static metadata
2+|Passed via `fields`

|===

A structured query to Splunk HEC looks like:

curl -k -v -X POST https://localhost:8080/services/collector/event/1.0 -H "Content-type: application/json; profile=\"urn:splunk:event:1.0\"; charset=utf-8" -H "Authorization: Splunk 29fe2838-cab6-4d17-a392-37b7b8f41f75" -d@events.json

.Nested serialization example
[source,json]
----
{
  "time": "1673001538.042",
  "host": "hostname",
  "source": "mysource",
  "sourcetype": "_json",
  "index": "main",
  "event": {
    "message": "2023-01-06 ERROR The log message",
    "logger": "com.acme.MyClass",
    "severity": "ERROR",
    "exception": "java.lang.NullPointerException",
    "properties": {
      "mdc-key": "mdc-value"
    }
  },
  "fields": {
    "key": "static-value"
  }
}
----

.Flat serialization example
[source,json]
----
{
  "time": "1673001538.042",
  "host": "hostname",
  "source": "mysource",
  "index": "main",
  "event": "2023-01-06 ERROR The log message",
  "fields": {
    "severity": "ERROR",
    "mdc-key": "mdc-value",
    "key": "static-value"
  }
}
----

=== Connectivity failures

Batched events that cannot be sent to the Splunk indexer will be logged to stdout:

* Formatted using console handler settings if the console handler is enabled
* Formatted using splunk handler settings otherwise

In any case, the root cause of the failure is always logged to stderr.

=== Asynchronous handler

By default, the log handler is synchronous and only the HTTP requests to HEC endpoint are done asynchronously:

[plantuml, sync, format=svg]
....
participant "Application" as App
participant Slf4j
participant SplunkLogHandler
participant "Splunk library" as Lib
participant "HTTP client" as OkHttp
participant "Splunk HEC" as HEC

group Application thread
App -> Slf4j: info(message)
Slf4j -> SplunkLogHandler: doPublish(record)
SplunkLogHandler -> Lib: send(record)
note left
  synchronized
end note
Lib -> Lib: Add event to batch
alt batch is full
Lib -> OkHttp: enqueue(HTTP request)
OkHttp --> Lib
end
Lib --> App

end
group HTTP client - multiple connections in parallel mode.
OkHttp -> OkHttp: Peek from queue
OkHttp -> HEC: HTTP POST /services/collector/event/1.0
HEC --> OkHttp: 200
alt status code != 200
OkHttp --> Lib: handle errors
Lib -> Lib: stderr.println
end
end
....

This can be an issue because the Splunk library `#send` is synchronized, so any preprocessing of the batch HTTP request itself happens on the application thread of the log event that triggered the batch to be full (either by reaching `quarkus.log.handler.splunk.batch-size-count` or `quarkus.log.handler.splunk.batch-size-bytes`)

By enabling `quarkus.log.handler.splunk.async=true`, an intermediate event queue is used, which decouples the flushing of the batch from any application thread:

[plantuml, async, format=svg]
....
participant "Application" as App
participant Slf4j
participant AsyncHandler
participant SplunkLogHandler
participant "Splunk library" as Lib
participant "HTTP client" as OkHttp
participant "Splunk HEC" as HEC

group Application thread
App -> Slf4j: info(message)
Slf4j -> AsyncHandler: doPublish(record)
AsyncHandler -> AsyncHandler: Capture MDC
AsyncHandler -> AsyncHandler: Add to queue
AsyncHandler --> App
end

group AsyncHandler single (daemon) thread
AsyncHandler -> AsyncHandler: Peek from queue
AsyncHandler -> SplunkLogHandler: doPublish(record)
SplunkLogHandler -> Lib: send(record)
note left
  synchronized
end note
Lib -> Lib: Add event to batch
alt batch is full
Lib -> OkHttp: enqueue(HTTP request)
OkHttp --> Lib
end
Lib --> SplunkLogHandler
SplunkLogHandler --> AsyncHandler

end
group HTTP client - multiple connections in parallel mode.
OkHttp -> OkHttp: Peek from queue
OkHttp -> HEC: HTTP POST /services/collector/event/1.0
HEC --> OkHttp: 200
alt status code != 200
OkHttp --> Lib: handle errors
Lib -> Lib: stderr.println
end
end
....

By default `quarkus.log.handler.splunk.async.queue-length=block`, so applicative threads will block once the queue limit has reached `quarkus.log.handler.splunk.async.queue-length`.

There's no link between `quarkus.log.handler.splunk.async.queue-length` and `quarkus.log.handler.splunk.batch-size-count`.

=== Sequential and parallel modes

The number of events kept in memory for batching purposes is not limited.
After tuning `quarkus.log.handler.splunk.batch-size-count` and `quarkus.log.handler.splunk.batch-size-bytes`, in case the HEC endpoint cannot keep up with the batch throughput, using multiple HTTP connections might help to reduce memory usage on the client.

By setting `quarkus.log.handler.splunk.send-mode=parallel` multiple batches will be sent over the wire in parallel, potentially increasing throughput with the HEC endpoint.

=== Named Splunk log handlers

A named log handler can be configured to manage multiple Splunk configurations for particular log emissions. Like for core Quarkus handlers (*console*, *file* or *syslog*),
Splunk named handlers follow the same configuration:

```
# Global configuration
quarkus.log.handler.splunk.token=12345678-1234-1234-1234-1234567890AB
quarkus.log.handler.splunk.metadata-index=mylogindex

# Splunk named handler configuration, named here MONITORING
quarkus.log.handler.splunk."MONITORING".token=12345678-0000-0000-0000-1234567890AB
quarkus.log.handler.splunk."MONITORING".metadata-index=mystatsindex

# Registration of the custom handler through Quarkus core category management, here monitoring as the logging category
quarkus.log.category."monitoring".handlers=MONITORING
quarkus.log.category."monitoring".use-parent-handlers=false
```

Next to use such logger in actual code, you can rely on annotation or factory:

* With annotation:
```
@LoggerName("monitoring")
Logger monitoringLogger;
```

* With factory:
```
static final Logger monitoringLogger = Logger.getLogger("monitoring");
```

==== Some important considerations
* Every handler is isolated and uses a separate Splunk client and connection pool, which means it has a cost.
* The configuration from the root handler are not inherited by named handlers.
* Use `quarkus.log.category."named-handler".use-parent-handlers=false` is required if you do not want the root handler to also receive log events already sent to named handlers.

== Extension Configuration Reference

This extension follows the `log handlers` configuration domain that is defined by Quarkus, every configuration property of this extension will belong to the following configuration root : `quarkus.log.handler.splunk`

When present this extension is enabled by default, meaning the client would expect a valid connection to a Splunk indexer and would print an error message for every log created by the application.

So in local environment, the log handler can be disabled with the following property :

[source,properties]
----
quarkus.log.handler.splunk.enabled=false
----

Every configuration property of the extension is overridable at runtime.

include::includes/quarkus-log-handler-splunk.adoc[leveloffset=+1, opts=optional]
